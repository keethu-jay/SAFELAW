{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Studio Baseline – Recalculated scores & Top 10 suggestion text\n",
    "\n",
    "**Why Tab sometimes shows no suggestions (0) then fills on retry:**  \n",
    "The first request can hit a cold start, timeout, or a brief backend/DB hiccup, so the UI shows nothing or 0. A second Tab sends a new request that completes, so you see all values in decreasing order. So it’s timing/transient, not a bug in the ordering. This notebook only records the top 10 suggestions and their semantic scores when **all 10 values are non-blank**.\n",
    "\n",
    "**This notebook:**  \n",
    "1. Reads your baseline CSV (first 7 columns unchanged).  \n",
    "2. Recalculates **Semantic Score Comparison of paragraphs** (test paragraph vs top suggestion).  \n",
    "3. Recalculates **Semantic Score Comparison of the whole file** (when XML data is available).  \n",
    "4. Swaps columns so **Semantic Scores** comes before **Top Suggestion**.  \n",
    "5. Adds **text + citation** for each of the top 10 suggestions (Suggestion 1 Text, Suggestion 1 Citation, …).  \n",
    "6. Only fills top 10 / scores when all 10 results are non-blank.  \n",
    "7. Output is **tab-separated (.tsv)** so commas in paragraphs don't split columns in Google Sheets (File > Import > Upload, choose Tab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q supabase isaacus numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set environment variables\n",
    "Paste your keys below (or use Colab secrets). You need: `SUPABASE_URL`, `SUPABASE_KEY`, `ISAACUS_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option A: paste keys here (don't commit!)\n",
    "os.environ[\"SUPABASE_URL\"] = \"\"  # e.g. https://xxx.supabase.co\n",
    "os.environ[\"SUPABASE_KEY\"] = \"\"   # service_role or anon key\n",
    "os.environ[\"ISAACUS_API_KEY\"] = \"\"\n",
    "\n",
    "# Option B: Colab secrets (recommended)\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"SUPABASE_URL\"] = userdata.get(\"SUPABASE_URL\")\n",
    "# os.environ[\"SUPABASE_KEY\"] = userdata.get(\"SUPABASE_KEY\")\n",
    "# os.environ[\"ISAACUS_API_KEY\"] = userdata.get(\"ISAACUS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload your CSV and optional data zip\n",
    "Upload the baseline CSV. For **Semantic Score Comparison of the whole file** we need the judgment XMLs; if you have them in a zip, upload it and set `DATA_ZIP_PATH` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "UPLOAD_CSV = True  # set False if CSV is already at INPUT_CSV_PATH\n",
    "INPUT_CSV_PATH = \"/content/Testing Corpus Studio - Baseline (Right Comparison).csv\"\n",
    "DATA_ZIP_PATH = None  # e.g. \"/content/data.zip\" after upload; must contain Final Dataset/ and raw_xml/ or similar\n",
    "\n",
    "if UPLOAD_CSV:\n",
    "    print(\"Upload your baseline CSV (Testing Corpus Studio - Baseline (Right Comparison).csv)\")\n",
    "    uploaded = files.upload()\n",
    "    for k in uploaded:\n",
    "        if k.endswith('.csv'):\n",
    "            INPUT_CSV_PATH = f\"/content/{k}\"\n",
    "            break\n",
    "\n",
    "BASE_DIR = Path(\"/content/data\")\n",
    "FINAL_DATASET_DIR = BASE_DIR / \"Final Dataset\"\n",
    "RAW_XML_DIR = BASE_DIR / \"raw_xml\"\n",
    "\n",
    "if DATA_ZIP_PATH and Path(DATA_ZIP_PATH).exists():\n",
    "    with zipfile.ZipFile(DATA_ZIP_PATH, 'r') as z:\n",
    "        z.extractall(\"/content\")\n",
    "    # Try common names\n",
    "    for name in [\"Final Dataset\", \"final_dataset\", \"Final Dataset/\", \"data\"]:\n",
    "        p = Path(\"/content\") / name.replace(\"/\", \"\")\n",
    "        if p.exists() and list(p.rglob("*.xml")):\n",
    "            FINAL_DATASET_DIR = p if p.is_dir() else p.parent\n",
    "            break\n",
    "    for name in [\"raw_xml\", \"data/raw_xml\"]:\n",
    "        p = Path(\"/content\") / name.replace(\"data/\", \"\")\n",
    "        if p.exists() and list(p.rglob("*.xml")):\n",
    "            RAW_XML_DIR = p\n",
    "            break\n",
    "\n",
    "print(\"CSV:\", INPUT_CSV_PATH)\n",
    "print(\"Final Dataset exists:\", FINAL_DATASET_DIR.exists() and bool(list(FINAL_DATASET_DIR.rglob(\"*.xml\"))))\n",
    "print(\"raw_xml exists:\", RAW_XML_DIR.exists() and bool(list(RAW_XML_DIR.rglob(\"*.xml\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helpers: CSV, embedding, XML, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def read_baseline_csv(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
    "        header = next(reader)\n",
    "        rows = list(reader)\n",
    "    return header, rows\n",
    "\n",
    "def normalize_case_name(s: str) -> str:\n",
    "    s = (s or \"\").strip().strip(\"()\").replace(\".xml\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).lower()\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def find_xml_file(case_name: str, search_dirs: list) -> Path:\n",
    "    raw = (case_name or \"\").strip().strip(\"()\")\n",
    "    if not raw:\n",
    "        return None\n",
    "    target_norm = normalize_case_name(raw)\n",
    "    target_prefix = target_norm.replace(\"...\", \"\").strip()\n",
    "    for base_dir in search_dirs:\n",
    "        base = Path(base_dir)\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for xml_path in base.rglob(\"*.xml\"):\n",
    "            stem_norm = normalize_case_name(xml_path.stem)\n",
    "            if stem_norm == target_norm or (target_prefix and stem_norm.startswith(target_prefix)):\n",
    "                return xml_path\n",
    "    return None\n",
    "\n",
    "def extract_text_from_element(el):\n",
    "    parts = [el.text.strip()] if el.text else []\n",
    "    for c in el:\n",
    "        parts.append(extract_text_from_element(c))\n",
    "        if c.tail:\n",
    "            parts.append(c.tail.strip())\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "\n",
    "def extract_full_text(file_path: Path) -> str:\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        parts = [t.strip() for t in root.itertext() if t and t.strip()]\n",
    "        return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))\n",
    "\n",
    "def embed_batch(client, model_name, texts, max_chars=8000):\n",
    "    if not texts:\n",
    "        return []\n",
    "    truncated = [t[:max_chars] + (\".\" * 3 if len(t) > max_chars else \"\") for t in texts if t and str(t).strip()]\n",
    "    if not truncated:\n",
    "        return [np.zeros(1792) for _ in texts]\n",
    "    try:\n",
    "        resp = client.embeddings.create(model=model_name, texts=truncated, task=\"retrieval/document\")\n",
    "        return [np.array(e.embedding) for e in resp.embeddings]\n",
    "    except Exception as e:\n",
    "        print(f\"  Embed failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_top_10_from_supabase(supabase, isaacus_client, model_name, test_paragraph: str, match_threshold=0.0):\n",
    "    if not (test_paragraph or test_paragraph.strip()):\n",
    "        return []\n",
    "    embs = embed_batch(isaacus_client, model_name, [test_paragraph.strip()[:8000]])\n",
    "    if not embs:\n",
    "        return []\n",
    "    try:\n",
    "        r = supabase.rpc(\n",
    "            \"match_corpus_documents\",\n",
    "            {\"query_embedding\": embs[0].tolist(), \"similarity_threshold\": match_threshold, \"max_results\": 10},\n",
    "        ).execute()\n",
    "        return r.data or []\n",
    "    except Exception as e:\n",
    "        print(f\"  RPC error: {e}\")\n",
    "        return []\n",
    "\n",
    "def compute_full_doc_similarity(path1: Path, path2: Path, client, model_name) -> float:\n",
    "    t1, t2 = extract_full_text(path1), extract_full_text(path2)\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    embs = embed_batch(client, model_name, [t1[:8000], t2[:8000]])\n",
    "    if len(embs) < 2:\n",
    "        return 0.0\n",
    "    return round(cosine_sim(embs[0], embs[1]), 4)\n",
    "\n",
    "print(\"Helpers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main: process each row and build output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "from isaacus import Isaacus\n",
    "\n",
    "header, rows = read_baseline_csv(INPUT_CSV_PATH)\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_KEY\")\n",
    "isaacus_key = os.environ.get(\"ISAACUS_API_KEY\")\n",
    "if not supabase_url or not supabase_key or not isaacus_key:\n",
    "    raise ValueError(\"Set SUPABASE_URL, SUPABASE_KEY, ISAACUS_API_KEY\")\n",
    "\n",
    "supabase = create_client(supabase_url, supabase_key)\n",
    "isaacus_client = Isaacus(api_key=isaacus_key)\n",
    "model_name = \"kanon-2-embedder\"\n",
    "\n",
    "search_dirs = [d for d in [FINAL_DATASET_DIR, RAW_XML_DIR] if d.exists()]\n",
    "\n",
    "suggestion_cols = []\n",
    "for i in range(1, 11):\n",
    "    suggestion_cols.append(f\"Suggestion {i} Text\")\n",
    "    suggestion_cols.append(f\"Suggestion {i} Citation\")\n",
    "out_header = [\n",
    "    header[0], header[1], header[2], header[3], header[4], header[5], header[6],  # first 7 unchanged\n",
    "    \"Semantic Score Comparison of paragraphs\",\n",
    "    \"Semantic Score Comparison of the whole file\",\n",
    "    \"Semantic Scores\",\n",
    "    \"Top Suggestion\",\n",
    "    *suggestion_cols,\n",
    "]\n",
    "\n",
    "output_rows = []\n",
    "for idx, row in enumerate(rows):\n",
    "    while len(row) < 11:\n",
    "        row.append(\"\")\n",
    "    court, opinion, file_name, position, ctx_prev, test_para, ctx_next = row[0], row[1], row[2], row[3], row[4], row[5], row[6]\n",
    "    first_7 = [court, opinion, file_name, position, ctx_prev, test_para, ctx_next]\n",
    "\n",
    "    top10 = get_top_10_from_supabase(supabase, isaacus_client, model_name, test_para, match_threshold=0.0)\n",
    "\n",
    "    para_sim = \"\"\n",
    "    full_doc_sim = \"\"\n",
    "    semantic_scores_col = \"\"\n",
    "    top_suggestion_col = \"\"\n",
    "    suggestion_texts = [\"\"] * 10\n",
    "    suggestion_citations = [\"\"] * 10\n",
    "\n",
    "    def _float_sim(hit, i):\n",
    "        v = hit.get(\"similarity\")\n",
    "        if isinstance(v, (int, float)): return float(v)\n",
    "        if isinstance(v, str):\n",
    "            try: return float(v)\n",
    "            except (TypeError, ValueError): pass\n",
    "        return None\n",
    "\n",
    "    if len(top10) >= 1:\n",
    "        first_sim = _float_sim(top10[0], 0)\n",
    "        if first_sim is not None:\n",
    "            para_sim = round(first_sim, 4)\n",
    "\n",
    "    if len(top10) >= 10:\n",
    "        scores = []\n",
    "        for i, hit in enumerate(top10[:10]):\n",
    "            sim = _float_sim(hit, i)\n",
    "            if sim is None:\n",
    "                scores = []\n",
    "                break\n",
    "            scores.append(sim)\n",
    "            suggestion_texts[i] = (hit.get(\"text\") or \"\").strip()\n",
    "            suggestion_citations[i] = (hit.get(\"doc_id\") or \"\").strip()\n",
    "\n",
    "        if len(scores) == 10:\n",
    "            semantic_scores_col = \", \".join(f\"{s:.4f}\" for s in scores)\n",
    "            first_text = (top10[0].get(\"text\") or \"\").strip()\n",
    "            doc_id = (top10[0].get(\"doc_id\") or \"\").strip()\n",
    "            top_suggestion_col = f\"{first_text} ({doc_id})\" if doc_id else first_text\n",
    "\n",
    "            path_source = find_xml_file(file_name.replace(\".xml\", \"\"), search_dirs) or find_xml_file(file_name, search_dirs)\n",
    "            path_sugg = find_xml_file(doc_id, search_dirs)\n",
    "            if path_source and path_sugg:\n",
    "                full_doc_sim = compute_full_doc_similarity(path_source, path_sugg, isaacus_client, model_name)\n",
    "\n",
    "    suggestion_pairs = []\n",
    "    for i in range(10):\n",
    "        suggestion_pairs.append(suggestion_texts[i])\n",
    "        suggestion_pairs.append(suggestion_citations[i])\n",
    "    output_rows.append(\n",
    "        first_7 + [str(para_sim) if para_sim != \"\" else \"\", str(full_doc_sim) if full_doc_sim != \"\" else \"\",\n",
    "                 semantic_scores_col, top_suggestion_col] + suggestion_pairs\n",
    "    )\n",
    "    print(f\"Row {idx+1}/{len(rows)}: para_sim={para_sim}, full_doc={full_doc_sim}, top10_filled={bool(semantic_scores_col)}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write CSV and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TSV = \"/content/Corpus_Studio_Baseline_Output.tsv\"\n",
    "\n",
    "with open(OUTPUT_TSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    w.writerow(out_header)\n",
    "    for r in output_rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "print(f\"Saved to {OUTPUT_TSV}\")\n",
    "print(\"In Google Sheets: File > Import > Upload, choose Tab as separator.\")\n",
    "from google.colab import files\n",
    "files.download(OUTPUT_TSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
